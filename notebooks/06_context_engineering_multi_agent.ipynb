{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 Context Engineering & Multi-Agent Orchestration\n",
    "\n",
    "This notebook demonstrates advanced **LangGraph** patterns using the IBM Granite model.\n",
    "\n",
    "We will build a **Multi-Agent System** with three specific nodes:\n",
    "1. **Context Engineer**: Analyzes the input to determine the appropriate \"tone\" and \"constraint\" (Context Engineering).\n",
    "2. **Supervisor**: Routes the task to the correct specialist based on the request.\n",
    "3. **Specialist Agents**: \n",
    "    - `MetricAgent`: Handles data and counting tasks.\n",
    "    - `CreativeAgent`: Handles writing and storytelling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import operator\n",
    "from typing import Annotated, List, TypedDict, Union\n",
    "\n",
    "# Setup paths\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Model\n",
    "llm = ChatOpenAI(\n",
    "    model=os.getenv(\"MODEL_NAME\", \"ibm-granite/granite-4.0-h-micro\"),\n",
    "    openai_api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    temperature=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define State & Context\n",
    "We define a `GraphState` that holds not just messages, but also specific **Context** variables that guide agent behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    # Context variables injected by the Context Engineer\n",
    "    tone: str\n",
    "    constraints: str\n",
    "    next_agent: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define The Nodes\n",
    "\n",
    "#### Node A: Context Engineer\n",
    "This node looks at the user input and decides *how* the agents should behave. This is \"Context Engineering\" ‚Äî separating the *how* from the *what*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_engineer_node(state: GraphState):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1].content\n",
    "    \n",
    "    # Prompt to extract context\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following user request: \"{last_message}\"\n",
    "    \n",
    "    Determine the appropriate TONE (e.g., Professional, Pirate, Robotic, Poetic) and CONSTRAINTS (e.g., Concise, Verbose, JSON-only).\n",
    "    \n",
    "    Respond in this exact format:\n",
    "    Tone: <tone>\n",
    "    Constraints: <constraints>\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt).content\n",
    "    \n",
    "    # Simple parsing (robustness depends on model)\n",
    "    tone = \"Professional\" # Default\n",
    "    constraints = \"None\" # Default\n",
    "    \n",
    "    for line in response.split('\\n'):\n",
    "        if line.startswith(\"Tone:\"):\n",
    "            tone = line.split(\"Tone:\")[1].strip()\n",
    "        if line.startswith(\"Constraints:\"):\n",
    "            constraints = line.split(\"Constraints:\")[1].strip()\n",
    "            \n",
    "    print(f\"üîç [Context Engineer] Detected Tone: {tone} | Constraints: {constraints}\")\n",
    "    return {\"tone\": tone, \"constraints\": constraints}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node B: Supervisor (Router)\n",
    "Decides which agent is best suited for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervisor_node(state: GraphState):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1].content\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a Supervisor. You have two workers:\n",
    "    1. MetricAgent: Good for counting, math, logic, and facts.\n",
    "    2. CreativeAgent: Good for writing, stories, poems, and ideas.\n",
    "    \n",
    "    User Request: \"{last_message}\"\n",
    "    \n",
    "    Respond with ONLY the name of the agent to handle this: 'MetricAgent' or 'CreativeAgent'.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt).content.strip()\n",
    "    \n",
    "    # Clean up response (remove punctuation/extra spaces)\n",
    "    next_agent = response.replace(\"'\", \"\").replace(\"\\\"\", \"\").strip()\n",
    "    \n",
    "    # Fallback\n",
    "    if \"Metric\" in next_agent: next_agent = \"MetricAgent\"\n",
    "    elif \"Creative\" in next_agent: next_agent = \"CreativeAgent\"\n",
    "    else: next_agent = \"CreativeAgent\" # Default\n",
    "    \n",
    "    print(f\"üëÆ [Supervisor] Routing to: {next_agent}\")\n",
    "    return {\"next_agent\": next_agent}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node C & D: The Workers\n",
    "These agents consume the `tone` and `constraints` from the state to generate their answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_agent_node(state: GraphState):\n",
    "    messages = state['messages']\n",
    "    tone = state['tone']\n",
    "    constraints = state['constraints']\n",
    "    \n",
    "    system_prompt = f\"\"\"\n",
    "    You are the MetricAgent. You focus on counting and hard facts.\n",
    "    \n",
    "    CONTEXT INSTRUCTIONS:\n",
    "    - Adopt this Tone: {tone}\n",
    "    - Follow Constraints: {constraints}\n",
    "    \"\"\"\n",
    "    \n",
    "    # We create a temporary message list for this generation\n",
    "    prompt_messages = [SystemMessage(content=system_prompt)] + messages\n",
    "    response = llm.invoke(prompt_messages)\n",
    "    \n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def creative_agent_node(state: GraphState):\n",
    "    messages = state['messages']\n",
    "    tone = state['tone']\n",
    "    constraints = state['constraints']\n",
    "    \n",
    "    system_prompt = f\"\"\"\n",
    "    You are the CreativeAgent. You focus on storytelling and ideation.\n",
    "    \n",
    "    CONTEXT INSTRUCTIONS:\n",
    "    - Adopt this Tone: {tone}\n",
    "    - Follow Constraints: {constraints}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt_messages = [SystemMessage(content=system_prompt)] + messages\n",
    "    response = llm.invoke(prompt_messages)\n",
    "    \n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Add Nodes\n",
    "workflow.add_node(\"context_engineer\", context_engineer_node)\n",
    "workflow.add_node(\"supervisor\", supervisor_node)\n",
    "workflow.add_node(\"metric_agent\", metric_agent_node)\n",
    "workflow.add_node(\"creative_agent\", creative_agent_node)\n",
    "\n",
    "# Set Entry Point\n",
    "workflow.set_entry_point(\"context_engineer\")\n",
    "\n",
    "# Edges\n",
    "# 1. Context Engineer -> Supervisor\n",
    "workflow.add_edge(\"context_engineer\", \"supervisor\")\n",
    "\n",
    "# 2. Supervisor -> Specific Agent (Conditional)\n",
    "def route_agents(state):\n",
    "    if state['next_agent'] == \"MetricAgent\":\n",
    "        return \"metric_agent\"\n",
    "    else:\n",
    "        return \"creative_agent\"\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    route_agents,\n",
    "    {\n",
    "        \"metric_agent\": \"metric_agent\",\n",
    "        \"creative_agent\": \"creative_agent\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# 3. Agents -> END\n",
    "workflow.add_edge(\"metric_agent\", END)\n",
    "workflow.add_edge(\"creative_agent\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run the Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query):\n",
    "    print(f\"\\nüöÄ User Query: \\\"{query}\\\"\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "    \n",
    "    inputs = {\"messages\": [HumanMessage(content=query)]}\n",
    "    \n",
    "    # Run the graph\n",
    "    result = app.invoke(inputs)\n",
    "    \n",
    "    final_msg = result['messages'][-1].content\n",
    "    print(\"\\nü§ñ Final Output:\")\n",
    "    print(final_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scenario A: Logic + Pirate Tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ User Query: \"Count the number of characters in the word 'Shipwreck' but say it like a pirate.\"\n",
      "--------------------------------------------------\n",
      "üîç [Context Engineer] Detected Tone: Pirate | Constraints: Concise\n",
      "üëÆ [Supervisor] Routing to: MetricAgent\n",
      "\n",
      "ü§ñ Final Output:\n",
      "Arr matey, the word 'Shipwreck' be holdin' 10 characters, me hearty!\n"
     ]
    }
   ],
   "source": [
    "run_query(\"Count the number of characters in the word 'Shipwreck' but say it like a pirate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scenario B: Creative + Robotic Tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ User Query: \"Write a very short poem about silicon chips, but keep it robotic and concise.\"\n",
      "--------------------------------------------------\n",
      "üîç [Context Engineer] Detected Tone: Robotic | Constraints: Concise\n",
      "üëÆ [Supervisor] Routing to: MetricAgent\n",
      "\n",
      "ü§ñ Final Output:\n",
      "Silicon, precise, in circuits we trust.\n"
     ]
    }
   ],
   "source": [
    "run_query(\"Write a very short poem about silicon chips, but keep it robotic and concise.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
